{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with word2vec\n",
    "\n",
    "The following experiments draw heavily from Ryan Heuser's experiments with [Word Vectors in the Eighteenth Century, Episode 1: Concepts](http://ryanheuser.org/word-vectors-1/). For more on word2vec, see his great introduction and prelimianry experiments with Edward Young's Riches are to Virtue as Learning is to Genius, as well the great post by [Ben Schmidt](http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html), and others many and various, who Ryan mentions in his introduction.\n",
    "\n",
    "#### Why am I interested in this word2vec stuff?\n",
    "\n",
    "My work focuses on the changing linguistic style of eighteenth-century British travel writing, from 1700 to 1830 (although I'm also working on an early modern corpus for the Early Modern Conversions project). For example, how did the use of first-person narration change? How can we track the development of aesthetic language in travel writing? How do subgenres develop later in the century, especially considering our received literary histories of divisions between personal narrative and guidebooks, for example?\n",
    "\n",
    "Okay, okay, but how is word2vec going to be useful in that endeavour? To be honest, I'm not exactly certain, but I have some ideas. The purpose of the following explorations are akin to brainstorming.\n",
    "\n",
    "First, if, like me, you want to use i/python to experiment, see [Ryan's page](http://ryanheuser.org/word-vectors-1/) once again (what a champ, right?) for his word2vec model that he trained on the ECCO-TCP corpus using gensim. As long as you have a working iPython installation, gensim installed, and that model, you should be good to go to follow along below or experiment on your own!\n",
    "\n",
    "So, let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('C:/Users/ASUS/Documents/iPython/word2vec.ECCO-TCP.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just do a few quick tests to see if things are working as we expect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does it recognize the same word as occupying the same vector space?\n",
    "model.similarity('woman', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87246097287540969"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are similar terms pretty similar?\n",
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('man', 0.8724610209465027)\n",
      "('girl', 0.8591000437736511)\n",
      "('gentleman', 0.7977981567382812)\n",
      "('creature', 0.7968071699142456)\n",
      "('gentlewoman', 0.7913839221000671)\n",
      "('person', 0.7872239351272583)\n",
      "('lover', 0.7776159048080444)\n",
      "('boy', 0.7674461603164673)\n",
      "('nobleman', 0.7542575597763062)\n",
      "('coquette', 0.7531524300575256)\n"
     ]
    }
   ],
   "source": [
    "# and checking the most_similar...\n",
    "print(*model.most_similar(['woman']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Everything looks good to go. Remember, this model was trained on the ECCO-TCP corpus, so if you're surprised by these or any of the other results, it may be because of the eighteenth century context.\n",
    "\n",
    "The most common example given of how word2vec works is the \"king - man + woman\" analogy. We know that the concept of a king, removed of its concept of man, but adding the concept of woman, should be queen, but what does the model think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('queen', 0.7854657173156738)\n",
      "('emperor', 0.7523161768913269)\n",
      "('prince', 0.7436755895614624)\n",
      "('princess', 0.7133168578147888)\n",
      "('conqueror', 0.7111818194389343)\n",
      "('regent', 0.7088087797164917)\n",
      "('empress', 0.6977599263191223)\n",
      "('sultan', 0.6729022264480591)\n",
      "('confessor', 0.6569845676422119)\n",
      "('duke', 0.6366889476776123)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['woman','king'], ['man']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! I'm not sure how to interpret some of thos eoter results - for example, why is emperor significantly higher than empress? - but let's keep those questions in mind as we push forward.\n",
    "\n",
    "### Travel\n",
    "\n",
    "Let's start with just a basic question: what other questions are associated with travel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ride', 0.8182368874549866)\n",
      "('wander', 0.7832238674163818)\n",
      "('go', 0.7742351293563843)\n",
      "('pass', 0.7557092905044556)\n",
      "('walk', 0.751531720161438)\n",
      "('run', 0.6949197053909302)\n",
      "('live', 0.6935456991195679)\n",
      "('steer', 0.6895339488983154)\n",
      "('swim', 0.6809810996055603)\n",
      "('advance', 0.6802776455879211)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['travel']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travel, here, is largely associated with verbs, rather than nouns. What if we add in some other elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('voyage', 0.8800842761993408)\n",
      "('tour', 0.7904102802276611)\n",
      "('route', 0.7388333082199097)\n",
      "('pilgrimage', 0.7387519478797913)\n",
      "('excursion', 0.713157057762146)\n",
      "('departure', 0.7096347212791443)\n",
      "('march', 0.6997677087783813)\n",
      "('journies', 0.6862897276878357)\n",
      "('travels', 0.6799992322921753)\n",
      "('expedition', 0.6781469583511353)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['journey']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, that is more of the kind of travel that I'm interested in - although I'm also intrigued by how the verbs of travel and the nouns of travel might intersect. What if we look at travel*s*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tour', 0.7089195251464844)\n",
      "('researches', 0.6972652673721313)\n",
      "('journey', 0.6799992918968201)\n",
      "('voyage', 0.6547428965568542)\n",
      "('rambles', 0.6496587991714478)\n",
      "('commentaries', 0.618072509765625)\n",
      "('pastorals', 0.616107165813446)\n",
      "('arrival', 0.612122654914856)\n",
      "('synopsis', 0.6118921637535095)\n",
      "('miscellanies', 0.6080632209777832)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['travels']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaah, very interesting. The numbers aren't as high as voyages, but the similarity of travels to terms like researches, commentaries, pastorals (?!), synopsis, and miscellanies may bear further fruit. \n",
    "*food for thought* \n",
    "- is there a a way to track how the vectors are changing over time?\n",
    "- if nothing else, this seems like a good way to populate word lists, for example if you want to do some machine learning on travel texts...\n",
    "\n",
    "For example, what if we take some of the highest ranked terms from our above journey experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arrival', 0.7368205785751343)\n",
      "('embassy', 0.7238683700561523)\n",
      "('expedition', 0.7216842174530029)\n",
      "('excursions', 0.7171652317047119)\n",
      "('entry', 0.7155791521072388)\n",
      "('retreat', 0.7114778757095337)\n",
      "('adventure', 0.699175238609314)\n",
      "('jaunt', 0.698330819606781)\n",
      "('travels', 0.6968503594398499)\n",
      "('passage', 0.6941586136817932)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['journey', 'voyage', 'tour', 'route', 'pilgrimage', 'excursion', 'departure', 'departure', 'excursion']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh! Travels still appears pretty low on the list. However, from my corpus building experiments with DREaM/EMC, \"travel\" as a title search term was a relatively reliable way to find texts about travel. But it looks like, outside of titles, other words occupy similar semantic space, with travel more on teh outskirts.\n",
    "\n",
    "#### What other questions can we ask about travel?\n",
    "\n",
    "Since my mind is on the gender track from earlier, what if we ask the same question about V(woman) + V(king) - V(man), but with travel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('voyage', 0.805957555770874)\n",
      "('tour', 0.7144608497619629)\n",
      "('pilgrimage', 0.6863122582435608)\n",
      "('march', 0.6787299513816833)\n",
      "('route', 0.6664501428604126)\n",
      "('travels', 0.6568373441696167)\n",
      "('excursion', 0.6390223503112793)\n",
      "('passage', 0.6379192471504211)\n",
      "('journies', 0.6373951435089111)\n",
      "('career', 0.6349523067474365)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['man','journey'], ['woman']), sep='\\n')\n",
    "# man + journey - woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('voyage', 0.7651640176773071)\n",
      "('tour', 0.6965904235839844)\n",
      "('departure', 0.6628884673118591)\n",
      "('route', 0.6525277495384216)\n",
      "('arrival', 0.636233925819397)\n",
      "('excursion', 0.6341248750686646)\n",
      "('pilgrimage', 0.632487416267395)\n",
      "('jaunt', 0.6280655860900879)\n",
      "('rambles', 0.6192945241928101)\n",
      "('retreat', 0.6117883920669556)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['woman','journey'], ['man']), sep='\\n')\n",
    "# woman _ journey - man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not an expert on gender and travel in the eighteenth century, but some of these differences seem notable - for example, march and career in men, and arrival, jaunt, rambles, and retreat in women. \n",
    "\n",
    "### differences in location\n",
    "\n",
    "Another question under consideration is how travel is conceptualized differently depending on whether one is traveling in, say, England, France, Africa, or the Caribbean. It *seems* like it should be a good way of teasing out some of the differences between these locations, but as you can see below, my experiments so far haven't been terribly successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('scotland', 0.9411507844924927)\n",
      "('france', 0.9026877880096436)\n",
      "('ireland', 0.8989571332931519)\n",
      "('spain', 0.8748767375946045)\n",
      "('holland', 0.8384954929351807)\n",
      "('europe', 0.8096485137939453)\n",
      "('italy', 0.7995690703392029)\n",
      "('america', 0.797402024269104)\n",
      "('poland', 0.7930401563644409)\n",
      "('portugal', 0.7820289731025696)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['england']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('asia', 0.8637418746948242)\n",
      "('peru', 0.8552894592285156)\n",
      "('siberia', 0.8434224724769592)\n",
      "('america', 0.8319177627563477)\n",
      "('persia', 0.8298490047454834)\n",
      "('mexico', 0.8288007974624634)\n",
      "('germany', 0.8287796974182129)\n",
      "('switzerland', 0.8209482431411743)\n",
      "('tartary', 0.8190882802009583)\n",
      "('russia', 0.8178454637527466)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['africa']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('voyage', 0.7463215589523315)\n",
      "('tour', 0.6522268056869507)\n",
      "('route', 0.5995424389839172)\n",
      "('coast', 0.5970306396484375)\n",
      "('travels', 0.5917956829071045)\n",
      "('coasts', 0.5887242555618286)\n",
      "('excursion', 0.5636414885520935)\n",
      "('north-east,', 0.5633217096328735)\n",
      "('journies', 0.5607912540435791)\n",
      "('frontiers', 0.5507453680038452)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['africa', 'journey'], ['england']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('voyage', 0.7463215589523315)\n",
      "('tour', 0.6522268056869507)\n",
      "('route', 0.5995424389839172)\n",
      "('coast', 0.5970306396484375)\n",
      "('travels', 0.5917956829071045)\n",
      "('coasts', 0.5887242555618286)\n",
      "('excursion', 0.5636414885520935)\n",
      "('north-east,', 0.5633217096328735)\n",
      "('journies', 0.5607912540435791)\n",
      "('frontiers', 0.5507453680038452)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['africa', 'journey'], ['england']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('specimen', 0.792880654335022)\n",
      "('representation', 0.7700939774513245)\n",
      "('sketch', 0.7623703479766846)\n",
      "('picture', 0.7562278509140015)\n",
      "('descriptions', 0.7303003072738647)\n",
      "('narration', 0.7277277708053589)\n",
      "('simile', 0.7254989147186279)\n",
      "('delineation', 0.7222810983657837)\n",
      "('definition', 0.7179701328277588)\n",
      "('narrative', 0.7146041393280029)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['description']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loveliness', 0.7935914993286133)\n",
      "('excellence', 0.7844477891921997)\n",
      "('elegance', 0.780930757522583)\n",
      "('softness', 0.7781577110290527)\n",
      "('charms', 0.7685889005661011)\n",
      "('sweetness', 0.7612348794937134)\n",
      "('splendour', 0.7471537590026855)\n",
      "('virtue', 0.7430456876754761)\n",
      "('splendor', 0.7428613901138306)\n",
      "('delicacy', 0.7419913411140442)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['beauty']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('whiteness', 0.6618044972419739)\n",
      "('richness', 0.6395145654678345)\n",
      "('fertility', 0.6272088289260864)\n",
      "('brilliancy', 0.6214094161987305)\n",
      "('sublimity', 0.62090665102005)\n",
      "('loveliness', 0.6144739389419556)\n",
      "('beauties', 0.6129385828971863)\n",
      "('brightness', 0.6119914054870605)\n",
      "('excellence', 0.610403835773468)\n",
      "('hues', 0.6103043556213379)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['africa', 'beauty'], ['england']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loveliness', 0.6491484045982361)\n",
      "('sublimity', 0.6464680433273315)\n",
      "('brightness', 0.6405013799667358)\n",
      "('whiteness', 0.6328679919242859)\n",
      "('splendor', 0.632349967956543)\n",
      "('splendour', 0.6321321725845337)\n",
      "('beauties', 0.6315566301345825)\n",
      "('grandeur', 0.6281092762947083)\n",
      "('hues', 0.6206766963005066)\n",
      "('splendors', 0.6132285594940186)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['asia', 'beauty'], ['england']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('excellence', 0.7609107494354248)\n",
      "('loveliness', 0.7390191555023193)\n",
      "('virtue', 0.7209703922271729)\n",
      "('sweetness', 0.7065260410308838)\n",
      "('merit', 0.6959548592567444)\n",
      "('sensibility', 0.694877564907074)\n",
      "('elegance', 0.6942763924598694)\n",
      "('softness', 0.6914743781089783)\n",
      "('charms', 0.6880205869674683)\n",
      "('beauties', 0.6824964880943298)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['england', 'beauty'], ['france']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('elegance', 0.7341766953468323)\n",
      "('softness', 0.7318840026855469)\n",
      "('splendour', 0.7203804850578308)\n",
      "('charms', 0.718169093132019)\n",
      "('loveliness', 0.7148070335388184)\n",
      "('splendor', 0.709806501865387)\n",
      "('grandeur', 0.6994746923446655)\n",
      "('delicacy', 0.6949677467346191)\n",
      "('brilliancy', 0.6926240921020508)\n",
      "('sweetness', 0.6878679990768433)\n"
     ]
    }
   ],
   "source": [
    "print(*model.most_similar(['france', 'beauty'], ['england']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['england'], ['england'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6154046656104969"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['england'], ['ireland'])\n",
    "0.61540466561049689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65522880572924469"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['england'], ['africa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of my problem, I think, is that I'm not thinking in terms of how the WEM is working - I expect the categories under beauty (so, descriptions of mountains) to show up, rather than just synonyms for beauty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
